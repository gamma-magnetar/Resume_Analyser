# -*- coding: utf-8 -*-
"""Resume Analyser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UlfJ5W1Eeg5_LrMN6AlvO3Oylpak1xjU
"""

!pip install pdfplumber python-docx reportlab openai

from IPython.display import display
from ipywidgets import FileUpload
import pdfplumber

# Step 1: Upload the file
upload = FileUpload(accept='.pdf', multiple=False)
display(upload)

"""# Uploaded resume is parsed and converted into json format"""

# supports pdf, doc and txt format
import pdfplumber
import docx

def extract_text_from_upload(uploaded_file):
    # Get uploaded file name and content
    name = list(uploaded_file.value.keys())[0]
    content = uploaded_file.value[name]['content']

    # Save the uploaded file locally
    with open(name, 'wb') as f:
        f.write(content)

    # Extract text based on file extension
    if name.endswith('.pdf'):
        with pdfplumber.open(name) as pdf:
            return "\n".join([page.extract_text() or "" for page in pdf.pages])
    elif name.endswith('.docx'):
        doc = docx.Document(name)
        return "\n".join([para.text for para in doc.paragraphs])
    elif name.endswith('.txt'):
        with open(name, 'r', encoding='utf-8') as f:
            return f.read()
    else:
        raise ValueError("Unsupported file format. Please upload PDF, DOCX, or TXT.")

# Use the function
resume_text = extract_text_from_upload(upload)

# Preview
print(resume_text[:2000])

import re
import json

def parse_resume_to_json(resume_text):
    sections = {
        "personal_info": {},
        "education": [],
        "experience": [],
        "skills": [],
        "projects": [],
        "certifications": [],
        "awards": [],
        "positions_of_responsibility": [],
        "extracurricular_activities": [],
        "summary": "",
    }

    # 1. Extract email and name (simple regex-based)
    email_match = re.search(r'[\w\.-]+@[\w\.-]+', resume_text)
    name_match = re.search(r'^[A-Z][a-z]+\s[A-Z][a-z]+', resume_text)

    sections["personal_info"]["email"] = email_match.group(0) if email_match else ""
    sections["personal_info"]["name"] = name_match.group(0) if name_match else ""

    # 2. Split based on headers
    lines = resume_text.splitlines()
    current_section = None

    for line in lines:
        clean_line = line.strip()

        # Match section headers
        section_map = {
            "education": ["education", "academic"],
            "experience": ["experience", "work experience", "professional experience"],
            "skills": ["skills", "technical skills"],
            "projects": ["projects"],
            "certifications": ["certifications"],
            "awards": ["awards", "achievements"],
            "positions_of_responsibility": ["positions of responsibility", "leadership"],
            "extracurricular_activities": ["extracurricular", "activities"],
            "summary": ["summary", "objective"]
        }

        matched_section = None
        for key, keywords in section_map.items():
            for keyword in keywords:
                if keyword.lower() in clean_line.lower():
                    matched_section = key
                    break
            if matched_section:
                break

        if matched_section:
            current_section = matched_section
            continue

        if current_section:
            if current_section in ["skills"]:
                sections[current_section].extend(re.split(r',|\n|;', clean_line))
            elif current_section == "summary":
                sections[current_section] += clean_line + " "
            else:
                sections[current_section].append(clean_line)

    return sections

resume_data = parse_resume_to_json(resume_text)

# Save to JSON file
with open("structured_resume.json", "w", encoding="utf-8") as f:
    json.dump(resume_data, f, indent=2)

print("✅ Resume has been parsed and saved to 'structured_resume.json'")

"""Saves the json file here and next you can download it from the uploads section."""

import json
import os

def save_resume_json(resume_data, output_path="structured_resume.json"):
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(resume_data, f, indent=2, ensure_ascii=False)
    print(f"✅ Structured resume saved to: {os.path.abspath(output_path)}")

# Example usage
resume_data = parse_resume_to_json(resume_text)  # your existing function
save_resume_json(resume_data)

"""# Prompt with gemini with result"""

import google.generativeai as genai

genai.configure(api_key="AIzaSyA1uYTXvYm_ivDI_wQfeDrPuuTybJZ5I9w")

#identifying sections
model = genai.GenerativeModel("gemini-1.5-flash")

def analyze_resume1(text):
    prompt = f"""You are an expert at analysing resume and giving expert feedbacks. Analyse the following resume content:

{text}

Tasks:
Please Identify and categorise sections like Summary, Skills, Education, Experience, etc.
"""
    response = model.generate_content(prompt)
    return response.text


result_identifying_sections = analyze_resume1(resume_text)
print(result_identifying_sections)

#detecting missing or underdeveloped sections
model = genai.GenerativeModel("gemini-1.5-flash")

def analyze_resume2(text):
    prompt = f"""You are an expert at analysing resume and giving expert feedbacks. Analyse the following resume content:

{text}

Tasks:
Detect missing or underdeveloped sections (e.g., no summary, sparse skills) with proper reasoning.
"""
    response = model.generate_content(prompt)
    return response.text


result_missing_sections = analyze_resume2(resume_text)
print(result_missing_sections)

#evaluate clarity....
model = genai.GenerativeModel("gemini-1.5-flash")

def analyze_resume3(text):
    prompt = f"""You are an expert at analysing resume and giving expert feedbacks. Analyse the following resume content:

{text}

Tasks:
Evaluate clarity, professionalism, and completeness of the resume.
"""
    response = model.generate_content(prompt)
    return response.text


result_clarity = analyze_resume3(resume_text)
print(result_clarity)

#evaluate sentiment....
model = genai.GenerativeModel("gemini-1.5-flash")

def analyze_resume4(text):
    prompt = f"""You are an expert at analysing resume and giving expert feedbacks. Analyse the following resume content:

{text}

Tasks:
Assess the sentiment of the skills section (e.g., confident, neutral, vague).
"""
    response = model.generate_content(prompt)
    return response.text


result_sentiment = analyze_resume4(resume_text)
print(result_sentiment)

#evaluate strengths....
model = genai.GenerativeModel("gemini-1.5-flash")

def analyze_resume6(text):
    prompt = f"""You are an expert at analysing resume and giving expert feedbacks. Analyse the following resume content:

{text}

Tasks:
Please Highlight strengths of the resume with reasoning (e.g., well-written sections or standout achievements).
"""
    response = model.generate_content(prompt)
    return response.text


result_strength = analyze_resume6(resume_text)
print(result_strength)

#Identify and flag excessive jargon or filler phrases
model = genai.GenerativeModel("gemini-1.5-flash")

def analyze_resume10(text):
    prompt = f"""You are an expert at analysing resume and giving expert feedbacks. Analyse the following resume content:

{text}

Tasks:
Please Identify and flag excessive jargon or filler phrases in our resume
"""
    response = model.generate_content(prompt)
    return response.text


result_jargon = analyze_resume10(resume_text)
print(result_jargon)

#ATS friendly formatting
model = genai.GenerativeModel("gemini-1.5-flash")

def analyze_resume11(text):
    prompt = f"""You are an expert at analysing resume and giving expert feedbacks. Analyse the following resume content:

{text}

Tasks:
Suggest ATS-friendly formatting (e.g., use of keywords, simple headings) improvements for our resume.
"""
    response = model.generate_content(prompt)
    return response.text


result_ats = analyze_resume11(resume_text)
print(result_ats)

#flagging jargons
model = genai.GenerativeModel("gemini-1.5-flash")

def analyze_resume10(text):
    prompt = f"""You are an expert at analysing resume and giving expert feedbacks. Analyse the following resume content:

{text}

Tasks:
Please Identify and flag excessive jargon or filler phrases in our resume
"""
    response = model.generate_content(prompt)
    return response.text


result_jargon = analyze_resume10(resume_text)
print(result_jargon)

#computing resume quality score
model = genai.GenerativeModel("gemini-1.5-flash")

def analyze_resume5(text):
    prompt = f"""You are an expert at analysing resume and giving expert resume quality score. Analyse the following resume content:

{text}

Also here are the insights on clarity, professionalism and completeness : {result_clarity}
Also here are the insights on sentiment of the skills section : {result_sentiment}
Also here are the insights on strengths of our resume : {result_strength}

Tasks:
Analyse our resume and the various insights provided on them to give a resume quality score based on the following 4 facctors. :
1. Section completenss
2. context richness
3. clarity and professionalism
4. Overall resume strength for role and year of experience

Also give score breakdown for each section.
"""
    response = model.generate_content(prompt)
    return response.text


result_score = analyze_resume5(resume_text)
print(result_score)

#evaluate strengths....
model = genai.GenerativeModel("gemini-1.5-flash")

def analyze_resume7(text):
    prompt = f"""You are an expert at analysing resume and giving expert feedbacks. Analyse the following resume content:

{text}

Tasks:
Please provide actionable improvement suggestions of our resume.
"""
    response = model.generate_content(prompt)
    return response.text


result_improv = analyze_resume7(resume_text)
print(result_improv)

#computing the final output
model = genai.GenerativeModel("gemini-1.5-flash", generation_config={"temperature": 0.3})

def analyze_resume8(text):
  prompt = f"""You are an expert resume reviewer.

Below is a resume followed by detailed feedback on various aspects of it. Your task is to synthesize this information and generate a final structured JSON summary.

Resume Content:
{text}

Feedback Insights:
- Missing and Underdeveloped Sections: {result_missing_sections}
- Clarity and Professionalism: {result_clarity}
- Skills Sentiment: {result_sentiment}
- Resume Quality Score: {result_score}
- Strengths: {result_strength}
- Suggestions for Improvement: {result_improv}
- Jargon and Filler Phrase Reduction: {result_jargon}
- ATS-Friendly Formatting: {result_ats}

Generate a JSON output in the following format (no explanations, no markdown):

{{
  "sections_detected": [...],
  "missing_sections": [...],
  "well_written_sections": [...],
  "resume_quality_score": <number>,
  "skills_sentiment_summary": "<string>",
  "improvement_suggestions": [...],
  "removing_jargon_recommendation": [...],
  "ats_formatting_recommendation": [...]
}}
"""
  response = model.generate_content(prompt)
  return response.text


result = analyze_resume8(resume_text)
print(result)

"""# Converting the result into format

The below code converts our text into json file which is downloadable from the side window
"""

import json

def clean_and_parse_json(response_text):
    # Step 1: Remove backticks and leading markdown tags
    cleaned = response_text.strip()

    # Remove code fences like ```json and ```
    if cleaned.startswith("```json"):
        cleaned = cleaned.removeprefix("```json").strip()
    elif cleaned.startswith("```"):
        cleaned = cleaned.removeprefix("```").strip()

    if cleaned.endswith("```"):
        cleaned = cleaned.removesuffix("```").strip()

    # Step 2: Now try parsing the cleaned string
    try:
        parsed = json.loads(cleaned)
        print("✅ JSON parsed successfully.")
        return parsed
    except json.JSONDecodeError as e:
        print("❌ JSON parsing failed:", e)
        print("Raw cleaned string was:\n", cleaned)
        return {}

json_data = clean_and_parse_json(result)

import json

def clean_result_string(result_text):
    """Remove markdown code formatting like ```json or ```."""
    result_text = result_text.strip()

    # Remove ```json or ```
    if result_text.startswith("```json"):
        result_text = result_text.removeprefix("```json").strip()
    elif result_text.startswith("```"):
        result_text = result_text.removeprefix("```").strip()

    if result_text.endswith("```"):
        result_text = result_text.removesuffix("```").strip()

    return result_text

# Step 1: Clean the result text
cleaned_result = clean_result_string(result)

# Step 2: Parse the string into a Python dict
try:
    json_data = json.loads(cleaned_result)
    print("✅ JSON parsed successfully.")
except json.JSONDecodeError as e:
    print("❌ Error parsing JSON:", e)
    json_data = {}

# Step 3: Save the data as a .json file
output_filename = "resume_analysis_output.json"
if json_data:
    with open(output_filename, "w", encoding="utf-8") as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)
    print(f"✅ JSON file saved as: {output_filename}")
else:
    print("⚠️ Skipped saving due to parsing failure.")

json_data

"""Below step coverts anlysis into pdf which is downloadable from side window"""

import json
from reportlab.lib.pagesizes import letter
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.units import inch

def generate_pdf_report(data, filename="resume_analysis_report.pdf"):
    doc = SimpleDocTemplate(filename, pagesize=letter)
    styles = getSampleStyleSheet()
    story = []

    story.append(Paragraph("Resume Analysis Report", styles['Title']))
    story.append(Spacer(1, 0.2 * inch))

    # Sections Detected
    story.append(Paragraph("<b>Sections Detected:</b>", styles['Heading3']))
    story.append(Paragraph(", ".join(data.get("sections_detected", [])), styles['Normal']))
    story.append(Spacer(1, 0.2 * inch))

    # Missing Sections
    story.append(Paragraph("<b>Missing Sections:</b>", styles['Heading3']))
    story.append(Paragraph(", ".join(data.get("missing_sections", [])) or "None", styles['Normal']))
    story.append(Spacer(1, 0.2 * inch))

    # Well Written Sections
    story.append(Paragraph("<b>Well-Written Sections:</b>", styles['Heading3']))
    story.append(Paragraph(", ".join(data.get("well_written_sections", [])), styles['Normal']))
    story.append(Spacer(1, 0.2 * inch))

    # Resume Score
    story.append(Paragraph("<b>Resume Quality Score:</b>", styles['Heading3']))
    story.append(Paragraph(str(data.get("resume_quality_score", "N/A")), styles['Normal']))
    story.append(Spacer(1, 0.2 * inch))

    # Sentiment Summary
    story.append(Paragraph("<b>Skills Sentiment Summary:</b>", styles['Heading3']))
    story.append(Paragraph(data.get("skills_sentiment_summary", ""), styles['Normal']))
    story.append(Spacer(1, 0.2 * inch))

    # Suggestions
    story.append(Paragraph("<b>Improvement Suggestions:</b>", styles['Heading3']))
    for suggestion in data.get("improvement_suggestions", []):
        story.append(Paragraph(f"- {suggestion}", styles['Normal']))
    story.append(Spacer(1, 0.2 * inch))

    # Jargon/Filler Suggestions
    story.append(Paragraph("<b>Jargon & Filler Reduction:</b>", styles['Heading3']))
    for tip in data.get("Removing jargon and filler phrases recommendation", []):
        story.append(Paragraph(f"- {tip}", styles['Normal']))
    story.append(Spacer(1, 0.2 * inch))

    # ATS Recommendations
    story.append(Paragraph("<b>ATS-Friendly Formatting Tips:</b>", styles['Heading3']))
    for tip in data.get("ATS Friendly formatting recommendation", []):
        story.append(Paragraph(f"- {tip}", styles['Normal']))
    story.append(Spacer(1, 0.2 * inch))

    doc.build(story)
    print(f"✅ PDF generated: {filename}")

# ---------- Clean and parse your raw text ----------
import re

# Suppose `result` is your full model output as a string
cleaned_result = re.sub(r"^```json\s*|\s*```$", "", result.strip(), flags=re.DOTALL)

# Now parse JSON
parsed_result = json.loads(cleaned_result)

# Generate PDF
generate_pdf_report(parsed_result)